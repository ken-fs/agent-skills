#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦å¤šç»´è¡¨æ ¼é›†æˆè„šæœ¬ - ä¸“ä¸šç‰ˆï¼ˆæ–¹æ¡ˆCï¼‰
ç”¨äºå°† Feynman å­¦ä¹ ç¬”è®°è‡ªåŠ¨è®°å½•åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼ï¼Œæ”¯æŒå®Œæ•´çš„å­¦ä¹ ç®¡ç†åŠŸèƒ½
"""

import os
import sys
import json
import requests
import re
import base64
import zlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆWindows å…¼å®¹ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
FEISHU_APP_ID = os.getenv("FEISHU_APP_ID", "")
FEISHU_APP_SECRET = os.getenv("FEISHU_APP_SECRET", "")
FEISHU_BITABLE_APP_TOKEN = os.getenv("FEISHU_BITABLE_APP_TOKEN", "")
FEISHU_BITABLE_TABLE_ID = os.getenv("FEISHU_BITABLE_TABLE_ID", "")

BASE_URL = "https://open.feishu.cn/open-apis"

# å¤ä¹ é—´éš”é…ç½®ï¼ˆåŸºäºé—å¿˜æ›²çº¿ï¼Œå•ä½ï¼šå¤©ï¼‰
REVIEW_INTERVALS = [1, 3, 7, 15, 30, 60]

# é»˜è®¤åˆ†ç±»æ ‡ç­¾é€‰é¡¹
DEFAULT_CATEGORIES = [
    "å‰ç«¯å¼€å‘", "åç«¯å¼€å‘", "ç®—æ³•ä¸æ•°æ®ç»“æ„", "æ•°æ®åº“",
    "ç½‘ç»œåè®®", "æ“ä½œç³»ç»Ÿ", "DevOps", "äº‘è®¡ç®—",
    "æ¶æ„è®¾è®¡", "AIä¸æœºå™¨å­¦ä¹ ", "ç§»åŠ¨å¼€å‘", "å®‰å…¨",
    "æµ‹è¯•", "å·¥å…·ä¸æ•ˆç‡", "å…¶ä»–"
]

# å®ŒæˆçŠ¶æ€é€‰é¡¹
COMPLETION_STATUS = {
    "learning": "ğŸŸ¡ å­¦ä¹ ä¸­",
    "mastered": "ğŸŸ¢ å·²æŒæ¡",
    "review": "ğŸ”µ éœ€å¤ä¹ ",
    "deep_dive": "ğŸŸ  å¾…æ·±å…¥",
    "archived": "âšª å·²å½’æ¡£"
}


def get_tenant_access_token() -> Optional[str]:
    """è·å– tenant_access_token"""
    url = f"{BASE_URL}/auth/v3/tenant_access_token/internal"
    payload = {
        "app_id": FEISHU_APP_ID,
        "app_secret": FEISHU_APP_SECRET
    }

    try:
        resp = requests.post(url, json=payload, timeout=10)
        data = resp.json()
        if data.get("code") == 0:
            return data.get("tenant_access_token")
        else:
            print(f"âŒ è·å– token å¤±è´¥: {data.get('msg')}")
            return None
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None


def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰

    Args:
        text: å¾…æå–çš„æ–‡æœ¬
        max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡

    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    # å¸¸è§æŠ€æœ¯å…³é”®è¯æ¨¡å¼
    tech_patterns = [
        r'\b[A-Z][a-zA-Z]+\b',  # å¤§å†™å¼€å¤´çš„è¯ï¼ˆå¦‚ React, Pythonï¼‰
        r'\b\w+(?:JS|js)\b',     # JSç›¸å…³ï¼ˆå¦‚ Node.js, Vue.jsï¼‰
        r'\b[A-Z]{2,}\b',        # å…¨å¤§å†™ç¼©å†™ï¼ˆå¦‚ API, HTTPï¼‰
        r'\b\w+-\w+\b',          # è¿å­—ç¬¦è¯ï¼ˆå¦‚ cross-platformï¼‰
    ]

    keywords = set()
    for pattern in tech_patterns:
        matches = re.findall(pattern, text)
        keywords.update(matches)

    # å¸¸è§æŠ€æœ¯è¯æ±‡
    common_tech_terms = [
        'async', 'await', 'promise', 'callback', 'function',
        'class', 'interface', 'type', 'module', 'component',
        'state', 'props', 'hook', 'context', 'reducer',
        'database', 'query', 'index', 'transaction', 'cache',
        'server', 'client', 'request', 'response', 'API',
        'frontend', 'backend', 'fullstack', 'microservice',
        'docker', 'kubernetes', 'deployment', 'CI/CD'
    ]

    text_lower = text.lower()
    for term in common_tech_terms:
        if term in text_lower:
            keywords.add(term.capitalize())

    # é™åˆ¶æ•°é‡å¹¶æ’åº
    return sorted(list(keywords))[:max_keywords]


def calculate_next_review(learning_date: datetime, review_count: int = 0) -> datetime:
    """
    è®¡ç®—ä¸‹æ¬¡å¤ä¹ æ—¶é—´ï¼ˆåŸºäºé—å¿˜æ›²çº¿ï¼‰

    Args:
        learning_date: å­¦ä¹ æ—¥æœŸ
        review_count: å·²å¤ä¹ æ¬¡æ•°

    Returns:
        ä¸‹æ¬¡å¤ä¹ çš„æ—¥æœŸæ—¶é—´
    """
    if review_count >= len(REVIEW_INTERVALS):
        # è¶…è¿‡é¢„è®¾é—´éš”ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªé—´éš”
        interval = REVIEW_INTERVALS[-1]
    else:
        interval = REVIEW_INTERVALS[review_count]

    return learning_date + timedelta(days=interval)


def auto_categorize(concept: str, content: str) -> List[str]:
    """
    æ ¹æ®æ¦‚å¿µå’Œå†…å®¹è‡ªåŠ¨åˆ†ç±»

    Args:
        concept: æ¦‚å¿µåç§°
        content: å­¦ä¹ å†…å®¹

    Returns:
        åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
    """
    categories = []
    combined_text = (concept + " " + content).lower()

    # åˆ†ç±»å…³é”®è¯æ˜ å°„
    category_keywords = {
        "å‰ç«¯å¼€å‘": ["react", "vue", "angular", "javascript", "typescript", "html", "css", "dom", "webpack", "å‰ç«¯"],
        "åç«¯å¼€å‘": ["node", "python", "java", "go", "rust", "api", "server", "åç«¯", "æœåŠ¡å™¨"],
        "ç®—æ³•ä¸æ•°æ®ç»“æ„": ["ç®—æ³•", "æ•°æ®ç»“æ„", "æ’åº", "æœç´¢", "æ ‘", "å›¾", "é“¾è¡¨", "æ ˆ", "é˜Ÿåˆ—", "å¤æ‚åº¦"],
        "æ•°æ®åº“": ["sql", "mysql", "postgresql", "mongodb", "redis", "æ•°æ®åº“", "æŸ¥è¯¢", "ç´¢å¼•"],
        "ç½‘ç»œåè®®": ["http", "tcp", "udp", "ip", "dns", "ç½‘ç»œ", "åè®®", "socket"],
        "æ“ä½œç³»ç»Ÿ": ["linux", "unix", "windows", "è¿›ç¨‹", "çº¿ç¨‹", "å†…å­˜", "æ“ä½œç³»ç»Ÿ"],
        "DevOps": ["docker", "kubernetes", "ci/cd", "jenkins", "gitlab", "devops", "éƒ¨ç½²"],
        "äº‘è®¡ç®—": ["aws", "azure", "gcp", "äº‘", "serverless", "lambda"],
        "æ¶æ„è®¾è®¡": ["æ¶æ„", "è®¾è®¡æ¨¡å¼", "å¾®æœåŠ¡", "åˆ†å¸ƒå¼", "é«˜å¯ç”¨", "è´Ÿè½½å‡è¡¡"],
        "AIä¸æœºå™¨å­¦ä¹ ": ["ai", "machine learning", "deep learning", "neural", "tensorflow", "pytorch", "äººå·¥æ™ºèƒ½"],
        "ç§»åŠ¨å¼€å‘": ["ios", "android", "react native", "flutter", "ç§»åŠ¨", "app"],
        "å®‰å…¨": ["security", "åŠ å¯†", "è®¤è¯", "æˆæƒ", "xss", "csrf", "å®‰å…¨"],
        "æµ‹è¯•": ["test", "testing", "unit test", "jest", "pytest", "æµ‹è¯•"],
    }

    for category, keywords in category_keywords.items():
        for keyword in keywords:
            if keyword in combined_text:
                categories.append(category)
                break  # æ¯ä¸ªåˆ†ç±»åªæ·»åŠ ä¸€æ¬¡

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•åˆ†ç±»ï¼Œæ·»åŠ "å…¶ä»–"
    if not categories:
        categories.append("å…¶ä»–")

    return categories


def save_feynman_note_pro(
    concept: str,
    simple_explanation: str,
    analogy: str,
    gaps: str,
    refined_explanation: str,
    key_takeaways: str,
    test_question: str,
    categories: Optional[List[str]] = None,
    mastery_level: int = 3,
    related_concepts: Optional[List[str]] = None,
    learning_duration: Optional[int] = None,
    completion_status: str = "learning",
    remaining_questions: str = "",
    mindmap_url: str = "",
    auto_extract_keywords: bool = True,
    auto_categorize_enabled: bool = True
) -> bool:
    """
    ä¿å­˜ Feynman ç¬”è®°åˆ°é£ä¹¦ï¼ˆä¸“ä¸šç‰ˆï¼‰

    Args:
        concept: æ¦‚å¿µåç§°
        simple_explanation: ç®€å•è§£é‡Š
        analogy: ç±»æ¯”
        gaps: çŸ¥è¯†ç©ºç™½
        refined_explanation: ç²¾ç‚¼è§£é‡Š
        key_takeaways: æ ¸å¿ƒè¦ç‚¹
        test_question: æµ‹è¯•é—®é¢˜
        categories: åˆ†ç±»æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œä¸ºç©ºåˆ™è‡ªåŠ¨åˆ†ç±»ï¼‰
        mastery_level: æŒæ¡ç¨‹åº¦ï¼ˆ1-5æ˜Ÿï¼Œé»˜è®¤3æ˜Ÿï¼‰
        related_concepts: ç›¸å…³æ¦‚å¿µåˆ—è¡¨
        learning_duration: å­¦ä¹ æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
        completion_status: å®ŒæˆçŠ¶æ€ï¼ˆlearning/mastered/review/deep_dive/archivedï¼‰
        remaining_questions: å¾…æ·±å…¥é—®é¢˜
        mindmap_url: æ€ç»´å¯¼å›¾é“¾æ¥
        auto_extract_keywords: æ˜¯å¦è‡ªåŠ¨æå–å…³é”®è¯
        auto_categorize_enabled: æ˜¯å¦è‡ªåŠ¨åˆ†ç±»

    Returns:
        bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    # éªŒè¯ç¯å¢ƒå˜é‡
    if not all([FEISHU_APP_ID, FEISHU_APP_SECRET, FEISHU_BITABLE_APP_TOKEN, FEISHU_BITABLE_TABLE_ID]):
        print("âŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡é…ç½®ï¼Œè¯·æ£€æŸ¥:")
        print(f"  FEISHU_APP_ID: {'âœ“' if FEISHU_APP_ID else 'âœ—'}")
        print(f"  FEISHU_APP_SECRET: {'âœ“' if FEISHU_APP_SECRET else 'âœ—'}")
        print(f"  FEISHU_BITABLE_APP_TOKEN: {'âœ“' if FEISHU_BITABLE_APP_TOKEN else 'âœ—'}")
        print(f"  FEISHU_BITABLE_TABLE_ID: {'âœ“' if FEISHU_BITABLE_TABLE_ID else 'âœ—'}")
        return False

    # è·å–è®¿é—®ä»¤ç‰Œ
    token = get_tenant_access_token()
    if not token:
        return False

    # è‡ªåŠ¨åˆ†ç±»
    if categories is None or (auto_categorize_enabled and not categories):
        categories = auto_categorize(concept, simple_explanation + " " + refined_explanation)

    # è‡ªåŠ¨æå–å…³é”®è¯
    ai_keywords = []
    if auto_extract_keywords:
        full_content = f"{concept} {simple_explanation} {refined_explanation}"
        ai_keywords = extract_keywords(full_content)

    # è®¡ç®—ä¸‹æ¬¡å¤ä¹ æ—¶é—´
    learning_date = datetime.now()
    next_review = calculate_next_review(learning_date, review_count=0)

    # æ„å»º API URL
    url = f"{BASE_URL}/bitable/v1/apps/{FEISHU_BITABLE_APP_TOKEN}/tables/{FEISHU_BITABLE_TABLE_ID}/records"

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }

    # æ„å»ºè®°å½•æ•°æ®ï¼ˆé€‚é…ç”¨æˆ·è¡¨æ ¼é…ç½®ï¼‰
    # æ³¨æ„ï¼šç”¨æˆ·çš„è¡¨æ ¼å­—æ®µç±»å‹ä¸ºå•è¡Œæ–‡æœ¬ï¼Œæ‰€ä»¥éœ€è¦æˆªæ–­é•¿æ–‡æœ¬
    def truncate_text(text: str, max_length: int = 5000) -> str:
        """æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”å•è¡Œæ–‡æœ¬å­—æ®µ"""
        return text[:max_length] if len(text) > max_length else text

    # ç”Ÿæˆ Mermaid mindmap è¯­æ³•å¹¶åˆ›å»ºåœ¨çº¿ URL
    def generate_mermaid_mindmap_url() -> str:
        """ç”Ÿæˆ Mermaid mindmap è¯­æ³•å¹¶è¿”å›åœ¨çº¿æŸ¥çœ‹ URL"""
        # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        def clean_text(text: str, max_len: int = 50) -> str:
            text = text.replace('"', "'").replace('\n', ' ').replace('\r', '')
            text = re.sub(r'\s+', ' ', text).strip()
            return text[:max_len] + "..." if len(text) > max_len else text

        # æ„å»º Mermaid mindmap è¯­æ³•
        mermaid_lines = ["mindmap"]
        mermaid_lines.append(f"  root(({clean_text(concept, 30)}))")

        # æ·»åŠ æ ¸å¿ƒç†è§£åˆ†æ”¯
        if simple_explanation:
            mermaid_lines.append("    æ ¸å¿ƒç†è§£")
            summary = clean_text(simple_explanation, 60)
            mermaid_lines.append(f"      {summary}")

        # æ·»åŠ ç±»æ¯”åˆ†æ”¯
        if analogy:
            mermaid_lines.append("    ç±»æ¯”è¯´æ˜")
            analogy_text = clean_text(analogy, 60)
            mermaid_lines.append(f"      {analogy_text}")

        # æ·»åŠ æ ¸å¿ƒè¦ç‚¹åˆ†æ”¯
        if key_takeaways:
            mermaid_lines.append("    æ ¸å¿ƒè¦ç‚¹")
            takeaway_lines = key_takeaways.split('\n')[:3]
            for line in takeaway_lines:
                if line.strip():
                    clean_line = clean_text(line.strip().lstrip('0123456789.-) '), 40)
                    if clean_line:
                        mermaid_lines.append(f"      {clean_line}")

        # æ·»åŠ çŸ¥è¯†ç©ºç™½åˆ†æ”¯
        if gaps:
            mermaid_lines.append("    çŸ¥è¯†ç©ºç™½")
            gap_lines = gaps.split('\n')[:2]
            for gap in gap_lines:
                if gap.strip():
                    clean_gap = clean_text(gap.strip(), 40)
                    mermaid_lines.append(f"      {clean_gap}")

        # æ·»åŠ å­¦ä¹ çŠ¶æ€åˆ†æ”¯
        mermaid_lines.append("    å­¦ä¹ çŠ¶æ€")
        mermaid_lines.append(f"      æŒæ¡: {'â­' * mastery_level}")
        status = COMPLETION_STATUS.get(completion_status, COMPLETION_STATUS['learning'])
        mermaid_lines.append(f"      çŠ¶æ€: {status}")

        # ç”Ÿæˆå®Œæ•´çš„ Mermaid ä»£ç 
        mermaid_code = '\n'.join(mermaid_lines)

        # åˆ›å»º Mermaid Live Editor URL
        # æ­£ç¡®çš„æ ¼å¼: https://mermaid.live/edit#pako:COMPRESSED_BASE64
        # ä½¿ç”¨ pako (zlib deflate) å‹ç¼©
        mermaid_config = {
            "code": mermaid_code,
            "mermaid": {"theme": "default"},
            "autoSync": True,
            "updateDiagram": True
        }

        try:
            # å°†é…ç½®è½¬ä¸º JSON å­—ç¬¦ä¸²
            json_str = json.dumps(mermaid_config, ensure_ascii=False)

            # ä½¿ç”¨ zlib deflate å‹ç¼©ï¼ˆpako å…¼å®¹æ ¼å¼ï¼‰
            compressed = zlib.compress(json_str.encode('utf-8'), level=9)[2:-4]  # å»é™¤ zlib å¤´å°¾

            # Base64 URL-safe ç¼–ç 
            encoded = base64.urlsafe_b64encode(compressed).decode('utf-8').rstrip('=')

            # ç”Ÿæˆåœ¨çº¿ URL
            online_url = f"https://mermaid.live/edit#pako:{encoded}"

            return online_url
        except Exception as e:
            # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œè¿”å›ç®€å•çš„æ–‡æœ¬è¯´æ˜
            print(f"ç”Ÿæˆæ€ç»´å¯¼å›¾ URL å¤±è´¥: {e}")
            return f"æ€ç»´å¯¼å›¾: {concept} (æŸ¥çœ‹å®Œæ•´ç¬”è®°è·å–è¯¦æƒ…)"

    # ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼ˆMarkdownæ ‘çŠ¶ç»“æ„ï¼Œç”¨äºå¤‡ä»½ï¼‰
    def generate_mindmap_markdown() -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ€ç»´å¯¼å›¾ç»“æ„ï¼ˆå¤‡ä»½æ–‡æœ¬ç‰ˆï¼‰"""
        mindmap_parts = [
            f"## æ€ç»´å¯¼å›¾: {concept}\n\n",
            f"### ğŸ“Œ ä¸­å¿ƒæ¦‚å¿µ\n",
            f"**{concept}**\n\n",
        ]

        # æ·»åŠ ç®€å•è§£é‡Šåˆ†æ”¯
        if simple_explanation:
            simple_summary = simple_explanation[:100] + "..." if len(simple_explanation) > 100 else simple_explanation
            mindmap_parts.append(f"### ğŸ¯ æ ¸å¿ƒç†è§£\n")
            mindmap_parts.append(f"- {simple_summary.replace(chr(10), ' ')}\n\n")

        # æ·»åŠ ç±»æ¯”åˆ†æ”¯
        if analogy:
            analogy_summary = analogy[:100] + "..." if len(analogy) > 100 else analogy
            mindmap_parts.append(f"### ğŸ”„ ç±»æ¯”\n")
            mindmap_parts.append(f"- {analogy_summary.replace(chr(10), ' ')}\n\n")

        # æ·»åŠ çŸ¥è¯†ç»“æ„
        if gaps:
            mindmap_parts.append(f"### ğŸ” çŸ¥è¯†ç©ºç™½\n")
            gap_lines = gaps.split('\n')[:3]  # æœ€å¤š3ä¸ªç©ºç™½
            for gap in gap_lines:
                if gap.strip():
                    mindmap_parts.append(f"- {gap.strip()[:80]}\n")
            mindmap_parts.append("\n")

        # æ·»åŠ æ ¸å¿ƒè¦ç‚¹
        if key_takeaways:
            mindmap_parts.append(f"### â­ æ ¸å¿ƒè¦ç‚¹\n")
            takeaway_lines = key_takeaways.split('\n')
            for line in takeaway_lines:
                if line.strip() and (line.strip()[0].isdigit() or line.strip().startswith('-')):
                    clean_line = line.strip().lstrip('0123456789.-) ')
                    if clean_line:
                        mindmap_parts.append(f"- {clean_line}\n")
            mindmap_parts.append("\n")

        # æ·»åŠ å…³é”®è¯äº‘
        if ai_keywords:
            mindmap_parts.append(f"### ğŸ”‘ å…³é”®è¯\n")
            mindmap_parts.append(f"`{' | '.join(ai_keywords[:8])}`\n\n")

        # æ·»åŠ å­¦ä¹ çŠ¶æ€
        mindmap_parts.append(f"### ğŸ“Š å­¦ä¹ çŠ¶æ€\n")
        mindmap_parts.append(f"- æŒæ¡ç¨‹åº¦: {'â­' * mastery_level}\n")
        mindmap_parts.append(f"- å®ŒæˆçŠ¶æ€: {COMPLETION_STATUS.get(completion_status, COMPLETION_STATUS['learning'])}\n")
        mindmap_parts.append(f"- åˆ†ç±»: {', '.join(categories)}\n")

        return "".join(mindmap_parts)

    # ç”Ÿæˆå®Œæ•´çš„Markdownæ ¼å¼å­¦ä¹ ç¬”è®°ï¼ˆç”¨äº"æ­£æ–‡å†…å®¹"å­—æ®µï¼‰
    def generate_full_content() -> str:
        """ç”Ÿæˆå®Œæ•´çš„Feynmanå­¦ä¹ ç¬”è®°ï¼ˆMarkdownæ ¼å¼ï¼‰"""
        content_parts = [
            f"# {concept}\n",
            f"**åˆ†ç±»**: {', '.join(categories)}\n",
            f"**æŒæ¡ç¨‹åº¦**: {'â­' * mastery_level}\n",
            f"**å®ŒæˆçŠ¶æ€**: {COMPLETION_STATUS.get(completion_status, COMPLETION_STATUS['learning'])}\n",
            "\n---\n",
            "\n## Step 1: ç®€å•è§£é‡Š\n",
            f"\n{simple_explanation}\n",
            "\n### ç±»æ¯”\n",
            f"\n{analogy}\n",
            "\n---\n",
            "\n## Step 2: çŸ¥è¯†ç©ºç™½\n",
            f"\n{gaps}\n",
            "\n---\n",
            "\n## Step 4: ç²¾ç‚¼è§£é‡Š\n",
            f"\n{refined_explanation}\n",
            "\n### æ ¸å¿ƒè¦ç‚¹\n",
            f"\n{key_takeaways}\n",
            "\n---\n",
            "\n## 30ç§’ç”µæ¢¯æµ‹è¯•\n",
            f"\n{test_question}\n"
        ]

        # å¦‚æœæœ‰AIå…³é”®è¯ï¼Œæ·»åŠ åˆ°ç¬”è®°ä¸­
        if ai_keywords:
            content_parts.append(f"\n**å…³é”®è¯**: {', '.join(ai_keywords[:10])}\n")

        return "".join(content_parts)

    # ç”Ÿæˆå®Œæ•´å†…å®¹å’Œæ€ç»´å¯¼å›¾
    full_content = generate_full_content()
    mindmap_markdown = generate_mindmap_markdown()  # ç”Ÿæˆ Markdown æ ¼å¼æ€ç»´å¯¼å›¾

    fields = {
        "æ¦‚å¿µ": concept,
        "æ­£æ–‡å†…å®¹": truncate_text(full_content),  # å®Œæ•´çš„Markdownç¬”è®°
        "åˆ†ç±»æ ‡ç­¾": categories,  # å¤šé€‰ç±»å‹ï¼Œæ­£å¸¸
        "ç®€å•è§£é‡Š": truncate_text(simple_explanation),  # å•è¡Œæ–‡æœ¬
        "ç±»æ¯”": truncate_text(analogy),  # å•è¡Œæ–‡æœ¬
        "çŸ¥è¯†ç©ºç™½": truncate_text(gaps),  # å•è¡Œæ–‡æœ¬
        "ç²¾ç‚¼è§£é‡Š": truncate_text(refined_explanation),  # å•è¡Œæ–‡æœ¬
        "æ ¸å¿ƒè¦ç‚¹": truncate_text(key_takeaways),  # å•è¡Œæ–‡æœ¬
        "æµ‹è¯•é—®é¢˜": truncate_text(test_question),  # å•è¡Œæ–‡æœ¬
        "æŒæ¡ç¨‹åº¦": mastery_level,  # è¯„åˆ†ç±»å‹ï¼Œä¼ å…¥æ•°å­— 1-5
        "å­¦ä¹ æ—¥æœŸ": int(learning_date.timestamp() * 1000),
        "å®ŒæˆçŠ¶æ€": COMPLETION_STATUS.get(completion_status, COMPLETION_STATUS["learning"]),
    }

    # æ·»åŠ å¯é€‰å­—æ®µï¼ˆé€‚é…å•è¡Œæ–‡æœ¬ï¼‰
    if ai_keywords:
        # AIå…³é”®è¯æ˜¯å•è¡Œæ–‡æœ¬ï¼Œç”¨é€—å·åˆ†éš”
        fields["AIå…³é”®è¯"] = ", ".join(ai_keywords[:10])  # æœ€å¤š10ä¸ªå…³é”®è¯

    # ä¿å­˜ Markdown æ ¼å¼çš„æ€ç»´å¯¼å›¾
    if mindmap_url:
        # å¦‚æœç”¨æˆ·æ‰‹åŠ¨æä¾›äº†æ€ç»´å¯¼å›¾URLï¼Œä¼˜å…ˆä½¿ç”¨
        fields["æ€ç»´å¯¼å›¾"] = mindmap_url
    else:
        # å¦åˆ™ä½¿ç”¨ Markdown æ ¼å¼çš„æ€ç»´å¯¼å›¾
        fields["æ€ç»´å¯¼å›¾"] = truncate_text(mindmap_markdown)

    # æ³¨æ„ï¼šä»¥ä¸‹å­—æ®µåœ¨ç”¨æˆ·è¡¨æ ¼ä¸­ä¸å­˜åœ¨ï¼Œå·²ç§»é™¤
    # - ä¸‹æ¬¡å¤ä¹ ï¼ˆä½†æˆ‘ä»¬ä»åœ¨åå°è®¡ç®—ï¼Œç”¨äºæ˜¾ç¤ºï¼‰
    # - å­¦ä¹ æ—¶é•¿
    # - å¾…æ·±å…¥é—®é¢˜

    # æ³¨æ„ï¼šç›¸å…³æ¦‚å¿µéœ€è¦è®°å½•IDï¼Œæš‚æ—¶ä¸æ”¯æŒè‡ªåŠ¨å…³è”
    # åç»­å¯ä»¥é€šè¿‡æŸ¥è¯¢å·²æœ‰è®°å½•æ¥å®ç°

    payload = {"fields": fields}

    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=15)
        data = resp.json()

        if data.get("code") == 0:
            record_id = data.get("data", {}).get("record", {}).get("record_id")
            print(f"\nâœ… Feynman å­¦ä¹ ç¬”è®°å·²ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼ï¼")
            print(f"   ğŸ“Œ æ¦‚å¿µ: {concept}")
            print(f"   ğŸ·ï¸  åˆ†ç±»: {', '.join(categories)}")
            print(f"   â­ æŒæ¡ç¨‹åº¦: {'â­' * mastery_level}")
            print(f"   ğŸ“… ä¸‹æ¬¡å¤ä¹ : {next_review.strftime('%Y-%m-%d')}")
            if ai_keywords:
                print(f"   ğŸ”‘ å…³é”®è¯: {', '.join(ai_keywords[:5])}{'...' if len(ai_keywords) > 5 else ''}")
            print(f"   ğŸ†” è®°å½•ID: {record_id}")
            print(f"   ğŸ”— æŸ¥çœ‹é“¾æ¥: https://my.feishu.cn/base/{FEISHU_BITABLE_APP_TOKEN}?table={FEISHU_BITABLE_TABLE_ID}&record={record_id}\n")
            return True
        else:
            print(f"âŒ ä¿å­˜å¤±è´¥: {data.get('msg')}")
            print(f"   é”™è¯¯ä»£ç : {data.get('code')}")
            if data.get('code') == 1254044:
                print(f"   ğŸ’¡ æç¤º: è¯·æ£€æŸ¥å¤šç»´è¡¨æ ¼å­—æ®µé…ç½®æ˜¯å¦ä¸è„šæœ¬åŒ¹é…")
                print(f"   ğŸ“– å‚è€ƒæ–‡æ¡£: feynman/references/table-setup-guide-pro.md")
            return False
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return False


def parse_feynman_markdown(markdown_content: str) -> Dict[str, Any]:
    """
    ä» Markdown æ ¼å¼çš„ Feynman ç¬”è®°ä¸­è§£ææ•°æ®

    Args:
        markdown_content: Markdown æ ¼å¼çš„å­¦ä¹ ç¬”è®°

    Returns:
        è§£æåçš„æ•°æ®å­—å…¸
    """
    data = {
        "concept": "",
        "simple_explanation": "",
        "analogy": "",
        "gaps": "",
        "refined_explanation": "",
        "key_takeaways": "",
        "test_question": "",
    }

    # æå–æ¦‚å¿µ
    concept_match = re.search(r'\*\*Concept\*\*:\s*\[(.*?)\]', markdown_content)
    if concept_match:
        data["concept"] = concept_match.group(1)

    # æå–ç®€å•è§£é‡Š
    simple_match = re.search(r'## Step 1: Explain It Simply\s*.*?### Simple Explanation\s*(.*?)(?=###|##|$)',
                            markdown_content, re.DOTALL)
    if simple_match:
        data["simple_explanation"] = simple_match.group(1).strip()

    # æå–ç±»æ¯”
    analogy_match = re.search(r'### Analogy\s*(.*?)(?=---|##|$)', markdown_content, re.DOTALL)
    if analogy_match:
        data["analogy"] = analogy_match.group(1).strip()

    # æå–çŸ¥è¯†ç©ºç™½
    gaps_match = re.search(r'## Step 2: Identify Gaps\s*(.*?)(?=##|$)', markdown_content, re.DOTALL)
    if gaps_match:
        data["gaps"] = gaps_match.group(1).strip()

    # æå–ç²¾ç‚¼è§£é‡Š
    refined_match = re.search(r'### Final Simple Explanation\s*(.*?)(?=###|##|$)', markdown_content, re.DOTALL)
    if refined_match:
        data["refined_explanation"] = refined_match.group(1).strip()

    # æå–æ ¸å¿ƒè¦ç‚¹
    takeaways_match = re.search(r'### Key Takeaways\s*(.*?)(?=---|##|$)', markdown_content, re.DOTALL)
    if takeaways_match:
        data["key_takeaways"] = takeaways_match.group(1).strip()

    # æå–æµ‹è¯•é—®é¢˜
    test_match = re.search(r'If someone asked me to explain this in 30 seconds.*?\n\s*>\s*(.*?)(?=##|$)',
                          markdown_content, re.DOTALL)
    if test_match:
        data["test_question"] = test_match.group(1).strip()

    return data


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        if sys.argv[1] == "--test":
            # æµ‹è¯•æ¨¡å¼
            print("ğŸ§ª è¿è¡Œ Feynman é£ä¹¦é›†æˆæµ‹è¯•ï¼ˆä¸“ä¸šç‰ˆï¼‰...\n")

            test_data = {
                "concept": "React Hooks æµ‹è¯•",
                "simple_explanation": "React Hooks æ˜¯ä¸€ç§è®©å‡½æ•°ç»„ä»¶ä¹Ÿèƒ½ä½¿ç”¨çŠ¶æ€å’Œå…¶ä»– React ç‰¹æ€§çš„æ–¹æ³•ã€‚",
                "analogy": "å°±åƒç»™æ™®é€šè‡ªè¡Œè½¦åŠ è£…äº†å˜é€Ÿå™¨å’Œåˆ¹è½¦ç³»ç»Ÿï¼Œè®©å®ƒæ‹¥æœ‰äº†å±±åœ°è½¦çš„åŠŸèƒ½ã€‚",
                "gaps": "1. Hook çš„åº•å±‚å®ç°åŸç†ä¸æ¸…æ¥š\n2. ä¸ºä»€ä¹ˆä¸èƒ½åœ¨æ¡ä»¶è¯­å¥ä¸­ä½¿ç”¨ Hook",
                "refined_explanation": "React Hooks æ˜¯ React 16.8 å¼•å…¥çš„ç‰¹æ€§ï¼Œè®©å‡½æ•°ç»„ä»¶èƒ½å¤Ÿä½¿ç”¨çŠ¶æ€ç®¡ç†ã€ç”Ÿå‘½å‘¨æœŸç­‰åŠŸèƒ½ã€‚æœ€å¸¸ç”¨çš„æ˜¯ useState å’Œ useEffectã€‚",
                "key_takeaways": "1. Hooks è®©ä»£ç æ›´ç®€æ´\n2. éµå¾ª Hooks è§„åˆ™å¾ˆé‡è¦\n3. è‡ªå®šä¹‰ Hook å¯ä»¥å¤ç”¨é€»è¾‘",
                "test_question": "React Hooks è®©å‡½æ•°ç»„ä»¶ä¹Ÿèƒ½ç®¡ç†çŠ¶æ€å’Œå‰¯ä½œç”¨ï¼Œæœ€å¸¸ç”¨çš„æ˜¯ useState å’Œ useEffectã€‚",
                "mastery_level": 4,
                "completion_status": "mastered",
                "learning_duration": 45,
                "remaining_questions": "Hook åœ¨ Fiber æ¶æ„ä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"
            }

            if save_feynman_note_pro(**test_data):
                print("âœ… æµ‹è¯•æˆåŠŸï¼è¯·æ£€æŸ¥ä½ çš„é£ä¹¦å¤šç»´è¡¨æ ¼ã€‚")
                print(f"ğŸ”— è¡¨æ ¼é“¾æ¥: https://my.feishu.cn/base/{FEISHU_BITABLE_APP_TOKEN}?table={FEISHU_BITABLE_TABLE_ID}")
            else:
                print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
                print("\nğŸ“‹ æ•…éšœæ’æŸ¥æ­¥éª¤:")
                print("1. æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®é…ç½®")
                print("2. ç¡®è®¤é£ä¹¦åº”ç”¨å·²å‘å¸ƒå¹¶æˆæƒ")
                print("3. éªŒè¯å¤šç»´è¡¨æ ¼å­—æ®µé…ç½®")
                print("4. æŸ¥çœ‹å‚è€ƒæ–‡æ¡£: feynman/references/table-setup-guide-pro.md")

        elif sys.argv[1] == "--parse" and len(sys.argv) > 2:
            # è§£æ Markdown æ–‡ä»¶
            file_path = sys.argv[2]
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                data = parse_feynman_markdown(content)
                print(f"âœ… å·²è§£ææ–‡ä»¶: {file_path}")
                print(json.dumps(data, indent=2, ensure_ascii=False))

                # è¯¢é—®æ˜¯å¦ä¿å­˜
                if input("\næ˜¯å¦ä¿å­˜åˆ°é£ä¹¦ï¼Ÿ(y/n): ").lower() == 'y':
                    save_feynman_note_pro(**data)

            except FileNotFoundError:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            except Exception as e:
                print(f"âŒ è§£æå¤±è´¥: {e}")

        else:
            print("ç”¨æ³•:")
            print("  python feishu_bitable_pro.py --test                    # è¿è¡Œæµ‹è¯•")
            print("  python feishu_bitable_pro.py --parse <æ–‡ä»¶è·¯å¾„>         # è§£æå¹¶ä¿å­˜ Markdown æ–‡ä»¶")
    else:
        print("ğŸ“ Feynman å­¦ä¹ ç¬”è®° - é£ä¹¦å¤šç»´è¡¨æ ¼é›†æˆï¼ˆä¸“ä¸šç‰ˆï¼‰")
        print("\nç”¨æ³•:")
        print("  python feishu_bitable_pro.py --test                    # è¿è¡Œæµ‹è¯•")
        print("  python feishu_bitable_pro.py --parse <æ–‡ä»¶è·¯å¾„>         # è§£æå¹¶ä¿å­˜ Markdown æ–‡ä»¶")
        print("\nåŠŸèƒ½ç‰¹æ€§:")
        print("  âœ… è‡ªåŠ¨åˆ†ç±»æ ‡ç­¾")
        print("  âœ… AI å…³é”®è¯æå–")
        print("  âœ… å¤ä¹ æé†’è®¡ç®—")
        print("  âœ… æŒæ¡ç¨‹åº¦è¯„åˆ†")
        print("  âœ… å­¦ä¹ æ—¶é•¿ç»Ÿè®¡")
        print("  âœ… å®Œæ•´æ•°æ®è¿½è¸ª")
        print("\né…ç½®æ–‡æ¡£:")
        print("  ğŸ“– feynman/references/table-setup-guide-pro.md")
        print("  ğŸ“– feynman/references/feishu-setup-guide.md")
